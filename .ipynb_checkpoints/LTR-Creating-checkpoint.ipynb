{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53a6bbce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 23004\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "with open('sentences_to_corrupt.txt', 'r') as f:\n",
    "    file = f.read()\n",
    "    split = file.split('.')\n",
    "\n",
    "proper_split_0 = []    \n",
    "for s in split:\n",
    "    if '?' in s:\n",
    "        ss = s.split('?')\n",
    "        for se in ss:\n",
    "            proper_split_0.append(se + '?')\n",
    "    else:\n",
    "        proper_split_0.append(s)\n",
    "\n",
    "proper_split = []    \n",
    "for s in proper_split_0:\n",
    "    if '!' in s:\n",
    "        ss = s.split('!')\n",
    "        for se in ss:\n",
    "            proper_split.append(se + '!')\n",
    "    else:\n",
    "        proper_split.append(s)\n",
    "        \n",
    "for p in proper_split:\n",
    "    if len(p.split()) <= 2: #removing sentences of 1 or 2 words because with 1 word, no switch between 2 words can be made,\n",
    "                            #and 2 not representative enough - there's no such case in the example data\n",
    "        proper_split.remove(p)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print('Number of sentences:', len(proper_split))\n",
    "\n",
    "# delimiters = \".\", \"?\", \"!\"\n",
    "# regexPattern = '|'.join(map(re.escape, delimiters))\n",
    "# f = re.split(regexPattern, file)\n",
    "# f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6cac91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using 1000 sentences from the dataset for time processing reasons\n",
    "split_1000 = proper_split[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "917fb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_shorter(sentence):\n",
    "    #!/usr/bin/env python3\n",
    "    \"\"\"Python 3 Example script for sending queries to the Sparv 2 API.\"\"\"\n",
    "\n",
    "    import json\n",
    "\n",
    "    # URL to the Sparv 2 API:\n",
    "    sparv_url = \"https://ws.spraakbanken.gu.se/ws/sparv/v2/\"\n",
    "\n",
    "    # The input text to be processed by Sparv:\n",
    "    sparv_input = str(sentence)\n",
    "\n",
    "    # Optional settings, specifying what analysis should be done (in this case compound analysis only):\n",
    "    # Check https://ws.spraakbanken.gu.se/ws/sparv/v2/#settings for more info\n",
    "    sparv_settings = json.dumps({\n",
    "        \"corpus\": \"Korpusnamn\",\n",
    "        \"lang\": \"sv\",\n",
    "        \"textmode\": \"plain\",\n",
    "        \"positional_attributes\": {\n",
    "            \"lexical_attributes\": [\n",
    "                \"pos\",\n",
    "                \"msd\",\n",
    "                \"lemma\"\n",
    "            ]\n",
    "        },\n",
    "        \"text_attributes\": {\n",
    "            \"readability_metrics\": [\n",
    "                \"lix\",\n",
    "                \"ovix\",\n",
    "                \"nk\"\n",
    "            ]\n",
    "        }\n",
    "    })\n",
    "\n",
    "    query_parameters = {\"text\": sparv_input, \"settings\": sparv_settings}\n",
    "\n",
    "\n",
    "    #-------------------------------------------------------------------\n",
    "    # Example using Python's built-in urllib.request\n",
    "    # https://docs.python.org/3/library/urllib.request.html\n",
    "    #-------------------------------------------------------------------\n",
    "\n",
    "    import urllib.request\n",
    "    import urllib.parse\n",
    "\n",
    "    enc_query = urllib.parse.urlencode(query_parameters).encode(\"UTF-8\")\n",
    "    resp = urllib.request.urlopen(sparv_url, data=enc_query).read()\n",
    "    \n",
    "#     print(\"\\nSparv response using urllib:\\n\")\n",
    "#     o = resp.decode(\"UTF-8\")\n",
    "#     print(o)\n",
    "\n",
    "    #-------------------------------------------------------------------\n",
    "    # Example using the more intuitive but third-pary Requests library\n",
    "    # https://docs.python-requests.org/en/master/\n",
    "    #-------------------------------------------------------------------\n",
    "\n",
    "    import requests\n",
    "\n",
    "#     response = requests.get(sparv_url, params=query_parameters)\n",
    "\n",
    "#     print(\"\\n\\nSparv response using the requests library:\\n\")\n",
    "#     print(response.text)\n",
    "    \n",
    "    return resp#, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11c7ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_n_words_maker(s):\n",
    "    pos = []\n",
    "    for line in s:\n",
    "        if line.startswith('<w pos='):\n",
    "            pos.append(line)\n",
    "    sentence_pos_n_words = []\n",
    "    for p in pos:\n",
    "        p_split = p.split()\n",
    "        if len(p_split) == 3:\n",
    "            lemma = p_split[2] #change to lemma\n",
    "            front = lemma[:lemma.index(\">\")] + '>'\n",
    "            back = lemma[lemma.index(\"<\"):]\n",
    "            nofront = lemma.replace(front,'')\n",
    "            nofrontnorback = nofront.replace(back, '')\n",
    "            pos_n_word = (p_split[1][5:7], nofrontnorback) # (pos tag, word)\n",
    "            sentence_pos_n_words.append(pos_n_word)\n",
    "        if len(p_split) == 4:\n",
    "            lemma = p_split[3]\n",
    "            front = lemma[:lemma.index(\">\")] + '>'\n",
    "            back = lemma[lemma.index(\"<\"):]\n",
    "            nofront = lemma.replace(front,'')\n",
    "            nofrontnorback = nofront.replace(back, '')\n",
    "            pos_n_word = (p_split[1][5:7], nofrontnorback) # (pos tag, word)\n",
    "            sentence_pos_n_words.append(pos_n_word)\n",
    "    return sentence_pos_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f8cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOO SLOW, SEE BELOW\n",
    "# import re\n",
    "\n",
    "# pos_sessed = []\n",
    "# for l in proper_split:\n",
    "#     ll, _ = pos_shorter(l)\n",
    "#     lldecoded = ll.decode(\"UTF-8\")\n",
    "#     s = lldecoded.splitlines()\n",
    "    \n",
    "#     p_n_w1 = pos_n_words_maker(s)\n",
    "#     pos_sessed.append(p_n_w1)\n",
    "# print(len(pos_sessed))\n",
    "# print(pos_sessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df40afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_in_parallel(l):\n",
    "    pos_sessed = []\n",
    "    ll = pos_shorter(l)\n",
    "    lldecoded = ll.decode(\"UTF-8\")\n",
    "    s = lldecoded.splitlines()\n",
    "\n",
    "    p_n_w1 = pos_n_words_maker(s)\n",
    "    pos_sessed.append(p_n_w1)\n",
    "    \n",
    "    return pos_sessed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7730896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOO SLOW --> when processing over 3000 sentences, speed increases dramatically\n",
    "\n",
    "# data = []\n",
    "# for i in range(0, 21884-1000, 1000):\n",
    "#     split_1000 = proper_split[i:i+1000]\n",
    "#     start = time.time()\n",
    "#     processed_data = Parallel(n_jobs = 10)(delayed(process_in_parallel)(l) for l in split_1000)\n",
    "#     data += processed_data\n",
    "#     end = time.time()\n",
    "\n",
    "#     print(\"Processing data took\", round(end-start, 0), \"seconds\")\n",
    "#     print(len(data), \"sentences processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98c94009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data took 7088.0 seconds\n",
      "1004 sentences processed\n"
     ]
    }
   ],
   "source": [
    "split_3000 = proper_split[22000:]\n",
    "start = time.time()\n",
    "processed_data = Parallel(n_jobs = 10)(delayed(process_in_parallel)(l) for l in split_3000)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Processing data took\", round(end-start, 0), \"seconds\")\n",
    "print(len(processed_data), \"sentences processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7973ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences containing verbs 876\n"
     ]
    }
   ],
   "source": [
    "#selecting sentences that contain verbs\n",
    "\n",
    "new_pos_sessed = []\n",
    "for bubu in processed_data:\n",
    "    for item in bubu:\n",
    "        if 'VB' in (i[0] for i in item):\n",
    "            new_pos_sessed.append(item)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "print('Number of sentences containing verbs', len(new_pos_sessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa80a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# save as json\n",
    "with open('sentences_to_corrupt_23000.json', 'w') as f:\n",
    "    json.dump(new_pos_sessed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "264e6797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685 corruptable sentences\n"
     ]
    }
   ],
   "source": [
    "# read the file\n",
    "to_corrupt = []\n",
    "with open('sentences_to_corrupt_3000.json') as f:\n",
    "    out = [(x) for x in json.load(f)]\n",
    "    for o in out:\n",
    "        coco = []\n",
    "        for oo in o:\n",
    "            ooo = (oo[0], oo[1])\n",
    "            coco.append(ooo)\n",
    "        to_corrupt.append(coco)\n",
    "\n",
    "print(len(to_corrupt), 'corruptable sentences')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
